{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "intro"
            },
            "source": [
                "# ðŸ”„ Incremental Data Update v3\n",
                "\n",
                "**Now checks an entire week of matches to catch any gaps.**\n",
                "\n",
                "âœ… Checks up to 20 pages of recent matches (~1 week per league)\n",
                "âœ… Uses match_id to skip already-known matches\n",
                "âœ… Handles Liga MX Apertura + Clausura"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "id": "install"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Ready!\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  WARNING: The scripts f2py.exe and numpy-config.exe are installed in 'C:\\Users\\Carlos\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\Scripts' which is not on PATH.\n",
                        "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
                        "\n",
                        "[notice] A new release of pip is available: 25.2 -> 25.3\n",
                        "[notice] To update, run: C:\\Users\\Carlos\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
                    ]
                }
            ],
            "source": [
                "!pip install tls_client pandas -q\n",
                "print('Ready!')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "id": "upload"
            },
            "outputs": [
                {
                    "ename": "ModuleNotFoundError",
                    "evalue": "No module named 'google'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#@title ðŸ“¤ Upload your existing data CSV\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mUpload your existing CSV:\u001b[39m\u001b[33m'\u001b[39m)\n",
                        "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google'"
                    ]
                }
            ],
            "source": [
                "#@title ðŸ“¤ Upload your existing data CSV\n",
                "from google.colab import files\n",
                "import pandas as pd\n",
                "\n",
                "print('Upload your existing CSV:')\n",
                "uploaded = files.upload()\n",
                "\n",
                "filename = list(uploaded.keys())[0]\n",
                "df_existing = pd.read_csv(filename)\n",
                "df_existing['date'] = pd.to_datetime(df_existing['date'])\n",
                "\n",
                "last_date = df_existing['date'].max()\n",
                "existing_ids = set(df_existing['match_id'].astype(int).tolist())\n",
                "\n",
                "print(f'\\nâœ… Loaded {len(df_existing):,} existing matches')\n",
                "print(f'ðŸ“… Last match date: {last_date.strftime(\"%Y-%m-%d\")}')\n",
                "print(f'ðŸ†” Known match IDs: {len(existing_ids):,}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "cellView": "form",
                "id": "config"
            },
            "outputs": [],
            "source": [
                "#@title Configuration { run: \"auto\" }\n",
                "#@markdown ### Leagues\n",
                "Premier_League = True #@param {type:\"boolean\"}\n",
                "La_Liga = True #@param {type:\"boolean\"}\n",
                "Bundesliga = True #@param {type:\"boolean\"}\n",
                "Serie_A = True #@param {type:\"boolean\"}\n",
                "Ligue_1 = True #@param {type:\"boolean\"}\n",
                "Liga_MX = True #@param {type:\"boolean\"}\n",
                "\n",
                "#@markdown ### Options\n",
                "Pages_To_Check = 20 #@param {type:\"slider\", min:5, max:50, step:5}\n",
                "Parallel_Workers = 5 #@param {type:\"slider\", min:2, max:10, step:1}\n",
                "\n",
                "LEAGUES = {\n",
                "    'Premier League': {'ids': [17]},\n",
                "    'La Liga': {'ids': [8]},\n",
                "    'Bundesliga': {'ids': [35]},\n",
                "    'Serie A': {'ids': [23]},\n",
                "    'Ligue 1': {'ids': [34]},\n",
                "    'Liga MX': {'ids': [11621, 11620]},\n",
                "}\n",
                "\n",
                "enabled = {'Premier League': Premier_League, 'La Liga': La_Liga,\n",
                "           'Bundesliga': Bundesliga, 'Serie A': Serie_A,\n",
                "           'Ligue 1': Ligue_1, 'Liga MX': Liga_MX}\n",
                "selected = {k: v for k, v in LEAGUES.items() if enabled.get(k, False)}\n",
                "print(f'Selected: {len(selected)} leagues | {Pages_To_Check} pages per league')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "client"
            },
            "outputs": [],
            "source": [
                "import time\n",
                "import random\n",
                "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
                "from threading import Lock\n",
                "from tls_client import Session\n",
                "from datetime import datetime, timedelta\n",
                "\n",
                "class SessionPool:\n",
                "    def __init__(self, size=5):\n",
                "        self.sessions = [Session(client_identifier='firefox_120') for _ in range(size)]\n",
                "        self.index = 0\n",
                "        self.lock = Lock()\n",
                "    def get(self):\n",
                "        with self.lock:\n",
                "            s = self.sessions[self.index % len(self.sessions)]\n",
                "            self.index += 1\n",
                "            return s\n",
                "\n",
                "pool = SessionPool(Parallel_Workers)\n",
                "BASE_URL = 'https://www.sofascore.com/api/v1'\n",
                "\n",
                "def fetch_json(url, retries=2):\n",
                "    full_url = f'{BASE_URL}{url}' if url.startswith('/') else url\n",
                "    session = pool.get()\n",
                "    for attempt in range(retries + 1):\n",
                "        try:\n",
                "            time.sleep(random.uniform(0.2, 0.5))\n",
                "            r = session.get(full_url)\n",
                "            if r.status_code == 200:\n",
                "                return r.json()\n",
                "            elif r.status_code == 403:\n",
                "                time.sleep(3)\n",
                "        except:\n",
                "            time.sleep(1)\n",
                "    return None\n",
                "\n",
                "def convert_fractional(frac_str):\n",
                "    try:\n",
                "        if '/' in str(frac_str):\n",
                "            n, d = map(int, str(frac_str).split('/'))\n",
                "            return round(1 + (n / d), 3)\n",
                "        return float(frac_str)\n",
                "    except:\n",
                "        return None\n",
                "\n",
                "print('Session pool ready')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "functions"
            },
            "outputs": [],
            "source": [
                "def get_finished_matches_full_scan(tournament_id, existing_ids, max_pages):\n",
                "    \"\"\"\n",
                "    FULL SCAN: Check ALL pages up to max_pages to find any missing matches.\n",
                "    Does NOT stop early - ensures we catch any gaps in data.\n",
                "    \"\"\"\n",
                "    # Get current season\n",
                "    seasons_data = fetch_json(f'/unique-tournament/{tournament_id}/seasons')\n",
                "    if not seasons_data or 'seasons' not in seasons_data:\n",
                "        return []\n",
                "    \n",
                "    season_id = seasons_data['seasons'][0]['id']\n",
                "    season_name = seasons_data['seasons'][0].get('name', 'Unknown')\n",
                "    print(f'    Season: {season_name}')\n",
                "    \n",
                "    new_matches = []\n",
                "    \n",
                "    for page in range(max_pages):\n",
                "        data = fetch_json(f'/unique-tournament/{tournament_id}/season/{season_id}/events/last/{page}')\n",
                "        \n",
                "        if not data or 'events' not in data:\n",
                "            print(f'    Page {page}: No more data')\n",
                "            break\n",
                "        \n",
                "        events = data.get('events', [])\n",
                "        if not events:\n",
                "            break\n",
                "        \n",
                "        page_new = 0\n",
                "        page_known = 0\n",
                "        for e in events:\n",
                "            match_id = e.get('id')\n",
                "            status_type = e.get('status', {}).get('type', '').lower()\n",
                "            \n",
                "            # Only finished matches\n",
                "            if status_type not in ['finished', 'ended']:\n",
                "                continue\n",
                "            \n",
                "            # Check if already known\n",
                "            if match_id in existing_ids:\n",
                "                page_known += 1\n",
                "                continue\n",
                "            \n",
                "            # New match found!\n",
                "            ts = e.get('startTimestamp', 0)\n",
                "            new_matches.append({\n",
                "                'match_id': match_id,\n",
                "                'date': datetime.fromtimestamp(ts).strftime('%Y-%m-%d'),\n",
                "                'timestamp': ts,\n",
                "                'home_team': e.get('homeTeam', {}).get('name'),\n",
                "                'home_team_id': e.get('homeTeam', {}).get('id'),\n",
                "                'away_team': e.get('awayTeam', {}).get('name'),\n",
                "                'away_team_id': e.get('awayTeam', {}).get('id'),\n",
                "                'home_score': e.get('homeScore', {}).get('current'),\n",
                "                'away_score': e.get('awayScore', {}).get('current'),\n",
                "                'tournament_id': tournament_id,\n",
                "            })\n",
                "            page_new += 1\n",
                "        \n",
                "        print(f'    Page {page}: +{page_new} new, {page_known} known')\n",
                "    \n",
                "    return new_matches\n",
                "\n",
                "\n",
                "def enrich_match(match):\n",
                "    \"\"\"Add stats, odds, H2H to a match.\"\"\"\n",
                "    match_id = match['match_id']\n",
                "    \n",
                "    # Stats\n",
                "    stats_data = fetch_json(f'/event/{match_id}/statistics')\n",
                "    if stats_data and 'statistics' in stats_data:\n",
                "        for period in stats_data.get('statistics', []):\n",
                "            pname = period.get('period', 'ALL').lower()\n",
                "            for g in period.get('groups', []):\n",
                "                for item in g.get('statisticsItems', []):\n",
                "                    name = item.get('name', '').lower().replace(' ', '_')\n",
                "                    key = name if pname == 'all' else f'{pname}_{name}'\n",
                "                    match[f'{key}_home'] = item.get('home')\n",
                "                    match[f'{key}_away'] = item.get('away')\n",
                "    \n",
                "    # Odds\n",
                "    odds_data = fetch_json(f'/event/{match_id}/odds/1/all')\n",
                "    if odds_data and 'markets' in odds_data:\n",
                "        for market in odds_data.get('markets', []):\n",
                "            mid = market.get('marketId')\n",
                "            for choice in market.get('choices', []):\n",
                "                name = choice.get('name', '')\n",
                "                dec = convert_fractional(choice.get('fractionalValue', ''))\n",
                "                if not dec:\n",
                "                    continue\n",
                "                if mid == 1:\n",
                "                    if name == '1': match['odds_1x2_home'] = dec\n",
                "                    elif name == 'X': match['odds_1x2_draw'] = dec\n",
                "                    elif name == '2': match['odds_1x2_away'] = dec\n",
                "                elif mid == 5:\n",
                "                    if name.lower() == 'yes': match['odds_btts_yes'] = dec\n",
                "                    elif name.lower() == 'no': match['odds_btts_no'] = dec\n",
                "    \n",
                "    # H2H\n",
                "    h2h_data = fetch_json(f'/event/{match_id}/h2h/events')\n",
                "    if h2h_data and 'events' in h2h_data:\n",
                "        h2h_home = h2h_away = h2h_draws = 0\n",
                "        for h2h in h2h_data.get('events', []):\n",
                "            if h2h.get('startTimestamp', 0) >= match['timestamp']:\n",
                "                continue\n",
                "            if h2h.get('status', {}).get('type') != 'finished':\n",
                "                continue\n",
                "            winner = h2h.get('winnerCode')\n",
                "            h2h_home_id = h2h.get('homeTeam', {}).get('id')\n",
                "            if winner == 1:\n",
                "                if h2h_home_id == match['home_team_id']: h2h_home += 1\n",
                "                else: h2h_away += 1\n",
                "            elif winner == 2:\n",
                "                if h2h_home_id == match['home_team_id']: h2h_away += 1\n",
                "                else: h2h_home += 1\n",
                "            elif winner == 3:\n",
                "                h2h_draws += 1\n",
                "        match['h2h_home_wins'] = h2h_home\n",
                "        match['h2h_away_wins'] = h2h_away\n",
                "        match['h2h_draws'] = h2h_draws\n",
                "    \n",
                "    return match\n",
                "\n",
                "print('Full scan functions ready')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "run"
            },
            "outputs": [],
            "source": [
                "#@title ðŸš€ Run Full Week Scan { display-mode: \"form\" }\n",
                "\n",
                "all_new = []\n",
                "start_time = time.time()\n",
                "\n",
                "print('='*60)\n",
                "print('INCREMENTAL UPDATE v3 - FULL WEEK SCAN')\n",
                "print('='*60)\n",
                "print(f'Known match IDs: {len(existing_ids):,}')\n",
                "print(f'Checking {Pages_To_Check} pages per league (~1 week of matches)\\n')\n",
                "\n",
                "for name, info in selected.items():\n",
                "    print(f'ðŸ” {name}')\n",
                "    \n",
                "    for tid in info['ids']:\n",
                "        matches = get_finished_matches_full_scan(tid, existing_ids, Pages_To_Check)\n",
                "        \n",
                "        if matches:\n",
                "            print(f'  âœ… Found {len(matches)} NEW matches\\n')\n",
                "            for m in matches:\n",
                "                m['league'] = name\n",
                "            all_new.extend(matches)\n",
                "            break\n",
                "        else:\n",
                "            print(f'  âšª No new matches\\n')\n",
                "\n",
                "print(f'ðŸ“Š Total NEW matches: {len(all_new)}')\n",
                "\n",
                "if all_new:\n",
                "    print(f'\\nâš™ï¸ Enriching {len(all_new)} matches...')\n",
                "    enriched = []\n",
                "    with ThreadPoolExecutor(max_workers=Parallel_Workers) as executor:\n",
                "        futures = [executor.submit(enrich_match, m) for m in all_new]\n",
                "        for i, future in enumerate(as_completed(futures)):\n",
                "            enriched.append(future.result())\n",
                "            print(f'\\r  Progress: {i+1}/{len(all_new)}', end='', flush=True)\n",
                "    \n",
                "    df_new = pd.DataFrame(enriched)\n",
                "    print(f'\\n\\nâœ… Enriched {len(df_new)} matches')\n",
                "    print(f'â±ï¸ Time: {(time.time() - start_time)/60:.1f} minutes')\n",
                "    \n",
                "    print('\\nðŸ“‹ New matches preview:')\n",
                "    display(df_new[['date', 'league', 'home_team', 'away_team', 'home_score', 'away_score']].head(20))\n",
                "else:\n",
                "    df_new = pd.DataFrame()\n",
                "    print('\\nâœ… No gaps found - your data is complete!')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "download"
            },
            "outputs": [],
            "source": [
                "#@title ðŸ“¥ Download Updated Data { display-mode: \"form\" }\n",
                "\n",
                "if len(df_new) > 0:\n",
                "    df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
                "    df_combined = df_combined.drop_duplicates(subset=['match_id'], keep='last')\n",
                "    df_combined = df_combined.sort_values('date').reset_index(drop=True)\n",
                "    \n",
                "    today = datetime.now().strftime('%Y%m%d')\n",
                "    out_filename = f'sofascore_updated_{len(df_combined)}matches_{today}.csv'\n",
                "    df_combined.to_csv(out_filename, index=False)\n",
                "    \n",
                "    print('='*60)\n",
                "    print('UPDATE COMPLETE!')\n",
                "    print('='*60)\n",
                "    print(f'Previous: {len(df_existing):,} matches')\n",
                "    print(f'Added:    {len(df_new):,} new matches')\n",
                "    print(f'Total:    {len(df_combined):,} matches')\n",
                "    print(f'\\nðŸ“… Date range: {df_combined[\"date\"].min()} to {df_combined[\"date\"].max()}')\n",
                "    \n",
                "    files.download(out_filename)\n",
                "else:\n",
                "    print('No new data to download.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "notes"
            },
            "source": [
                "---\n",
                "## v3 Changes\n",
                "- **Full scan**: Checks ALL pages up to limit (no early stopping)\n",
                "- **Configurable**: Slider to set pages (20 = ~1 week per league)\n",
                "- **Gap detection**: Finds missing matches even if surrounded by known ones"
            ]
        }
    ],
    "metadata": {
        "colab": {
            "provenance": [],
            "toc_visible": true
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
