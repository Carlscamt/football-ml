{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ‚ö° Sofascore Multi-League Scraper v4 (PARALLEL)\n",
        "\n",
        "**Fast parallel scraper with:**\n",
        "- üöÄ Multi-threaded API calls (5-10x faster)\n",
        "- üåç 10+ major world leagues\n",
        "- üìÖ 3 years of historical data\n",
        "- üí∞ Full odds (1X2, BTTS, O/U)\n",
        "- üìä Team streaks & H2H"
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "install",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6881630d-bf4d-4862-d428-be0a6ff0d44c",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m41.3/41.3 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDependencies installed!\n"
          ]
        }
      ],
      "source": [
        "!pip install tls_client pandas numpy -q\n",
        "print(\"Dependencies installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Configuration { run: \"auto\" }\n",
        "#@markdown ### Leagues:\n",
        "Premier_League = True #@param {type:\"boolean\"}\n",
        "La_Liga = True #@param {type:\"boolean\"}\n",
        "Bundesliga = True #@param {type:\"boolean\"}\n",
        "Serie_A = True #@param {type:\"boolean\"}\n",
        "Ligue_1 = True #@param {type:\"boolean\"}\n",
        "Liga_MX = True #@param {type:\"boolean\"}\n",
        "Eredivisie = False #@param {type:\"boolean\"}\n",
        "Primeira_Liga = False #@param {type:\"boolean\"}\n",
        "MLS = False #@param {type:\"boolean\"}\n",
        "Brazilian_Serie_A = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ### Options:\n",
        "Years_of_Data = 3 #@param {type:\"slider\", min:1, max:5, step:1}\n",
        "Max_Matches_Per_Team = 30 #@param {type:\"slider\", min:10, max:50, step:5}\n",
        "Parallel_Workers = 10 #@param {type:\"slider\", min:2, max:10, step:1}\n",
        "Include_Odds = True #@param {type:\"boolean\"}\n",
        "Include_Streaks = True #@param {type:\"boolean\"}\n",
        "Include_H2H = True #@param {type:\"boolean\"}\n",
        "\n",
        "LEAGUES = {\n",
        "    'Premier League': {'id': 17, 'enabled': Premier_League},\n",
        "    'La Liga': {'id': 8, 'enabled': La_Liga},\n",
        "    'Bundesliga': {'id': 35, 'enabled': Bundesliga},\n",
        "    'Serie A': {'id': 23, 'enabled': Serie_A},\n",
        "    'Ligue 1': {'id': 34, 'enabled': Ligue_1},\n",
        "    'Liga MX': {'id': 11621, 'enabled': Liga_MX},\n",
        "    'Eredivisie': {'id': 37, 'enabled': Eredivisie},\n",
        "    'Primeira Liga': {'id': 238, 'enabled': Primeira_Liga},\n",
        "    'MLS': {'id': 242, 'enabled': MLS},\n",
        "    'Brazilian Serie A': {'id': 325, 'enabled': Brazilian_Serie_A},\n",
        "}\n",
        "\n",
        "selected = {k: v for k, v in LEAGUES.items() if v['enabled']}\n",
        "print(f\"Selected: {len(selected)} leagues | {Years_of_Data} years | {Parallel_Workers} workers\")"
      ],
      "metadata": {
        "id": "config",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5e17719-d87b-4bfc-afcd-da6fc4ebebfa",
        "cellView": "form"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected: 6 leagues | 3 years | 10 workers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "import re\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from threading import Lock\n",
        "from tls_client import Session\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "# Thread-safe session pool\n",
        "class SessionPool:\n",
        "    def __init__(self, size=5):\n",
        "        self.sessions = [Session(client_identifier=\"firefox_120\") for _ in range(size)]\n",
        "        self.index = 0\n",
        "        self.lock = Lock()\n",
        "\n",
        "    def get(self):\n",
        "        with self.lock:\n",
        "            session = self.sessions[self.index % len(self.sessions)]\n",
        "            self.index += 1\n",
        "            return session\n",
        "\n",
        "pool = SessionPool(Parallel_Workers)\n",
        "BASE_URL = \"https://www.sofascore.com/api/v1\"\n",
        "request_count = 0\n",
        "count_lock = Lock()\n",
        "\n",
        "def fetch_json(url, retries=2):\n",
        "    global request_count\n",
        "    full_url = f\"{BASE_URL}{url}\" if url.startswith('/') else url\n",
        "    session = pool.get()\n",
        "\n",
        "    for attempt in range(retries + 1):\n",
        "        try:\n",
        "            time.sleep(random.uniform(0.2, 0.5))  # Reduced delay for parallel\n",
        "            response = session.get(full_url)\n",
        "\n",
        "            with count_lock:\n",
        "                request_count += 1\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                return response.json()\n",
        "            elif response.status_code == 403:\n",
        "                time.sleep(3)  # Back off on 403\n",
        "        except:\n",
        "            if attempt < retries:\n",
        "                time.sleep(1)\n",
        "    return None\n",
        "\n",
        "def convert_fractional(frac_str):\n",
        "    try:\n",
        "        if '/' in str(frac_str):\n",
        "            num, den = map(int, str(frac_str).split('/'))\n",
        "            return round(1 + (num / den), 3)\n",
        "        return float(frac_str)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def slugify(text):\n",
        "    return re.sub(r'[^a-z0-9]+', '_', text.lower()).strip('_')\n",
        "\n",
        "print(f\"Session pool ready with {Parallel_Workers} workers\")"
      ],
      "metadata": {
        "id": "client",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15351e9a-331a-47f3-8347-3799758b3462"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Session pool ready with 10 workers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data fetching functions (optimized for parallel use)\n",
        "\n",
        "def get_seasons(tournament_id, num_years=3):\n",
        "    data = fetch_json(f\"/unique-tournament/{tournament_id}/seasons\")\n",
        "    if not data or 'seasons' not in data:\n",
        "        return []\n",
        "    return [{'id': s['id'], 'name': s['name']} for s in data['seasons'][:num_years * 2]]\n",
        "\n",
        "def get_teams(tournament_id, season_id):\n",
        "    data = fetch_json(f\"/unique-tournament/{tournament_id}/season/{season_id}/standings/total\")\n",
        "    if not data or 'standings' not in data:\n",
        "        return []\n",
        "    teams = []\n",
        "    for s in data['standings']:\n",
        "        for r in s.get('rows', []):\n",
        "            t = r.get('team', {})\n",
        "            teams.append({'id': t.get('id'), 'name': t.get('name')})\n",
        "    return teams\n",
        "\n",
        "def get_team_matches(team_id, tournament_id, max_pages=3):\n",
        "    matches = []\n",
        "    for page in range(max_pages):\n",
        "        data = fetch_json(f\"/team/{team_id}/events/last/{page}\")\n",
        "        if not data or 'events' not in data:\n",
        "            break\n",
        "        for e in data.get('events', []):\n",
        "            if e.get('status', {}).get('type') != 'finished':\n",
        "                continue\n",
        "            if e.get('tournament', {}).get('uniqueTournament', {}).get('id') != tournament_id:\n",
        "                continue\n",
        "            matches.append({\n",
        "                'match_id': e.get('id'),\n",
        "                'date': datetime.fromtimestamp(e.get('startTimestamp', 0)).strftime('%Y-%m-%d'),\n",
        "                'timestamp': e.get('startTimestamp', 0),\n",
        "                'home_team': e.get('homeTeam', {}).get('name'),\n",
        "                'home_team_id': e.get('homeTeam', {}).get('id'),\n",
        "                'away_team': e.get('awayTeam', {}).get('name'),\n",
        "                'away_team_id': e.get('awayTeam', {}).get('id'),\n",
        "                'home_score': e.get('homeScore', {}).get('current'),\n",
        "                'away_score': e.get('awayScore', {}).get('current'),\n",
        "                'tournament_id': tournament_id,\n",
        "            })\n",
        "    return matches\n",
        "\n",
        "def enrich_match(match, include_odds, include_streaks, include_h2h):\n",
        "    \"\"\"Enrich a single match with stats, odds, streaks, h2h.\"\"\"\n",
        "    match_id = match['match_id']\n",
        "\n",
        "    # Stats\n",
        "    stats_data = fetch_json(f\"/event/{match_id}/statistics\")\n",
        "    if stats_data and 'statistics' in stats_data:\n",
        "        for period in stats_data.get('statistics', []):\n",
        "            pname = period.get('period', 'ALL').lower()\n",
        "            for g in period.get('groups', []):\n",
        "                for item in g.get('statisticsItems', []):\n",
        "                    name = item.get('name', '').lower().replace(' ', '_')\n",
        "                    key = name if pname == 'all' else f\"{pname}_{name}\"\n",
        "                    match[f\"{key}_home\"] = item.get('home')\n",
        "                    match[f\"{key}_away\"] = item.get('away')\n",
        "\n",
        "    # Odds\n",
        "    if include_odds:\n",
        "        odds_data = fetch_json(f\"/event/{match_id}/odds/1/all\")\n",
        "        if odds_data and 'markets' in odds_data:\n",
        "            for market in odds_data.get('markets', []):\n",
        "                mid = market.get('marketId')\n",
        "                for choice in market.get('choices', []):\n",
        "                    name = choice.get('name', '')\n",
        "                    dec = convert_fractional(choice.get('fractionalValue', ''))\n",
        "                    if not dec: continue\n",
        "                    if mid == 1:\n",
        "                        if name == '1': match['odds_1x2_home'] = dec\n",
        "                        elif name == 'X': match['odds_1x2_draw'] = dec\n",
        "                        elif name == '2': match['odds_1x2_away'] = dec\n",
        "                    elif mid == 5:\n",
        "                        if name.lower() == 'yes': match['odds_btts_yes'] = dec\n",
        "                        elif name.lower() == 'no': match['odds_btts_no'] = dec\n",
        "\n",
        "    # Streaks\n",
        "    if include_streaks:\n",
        "        streak_data = fetch_json(f\"/event/{match_id}/team-streaks\")\n",
        "        if streak_data:\n",
        "            for item in streak_data.get('general', []):\n",
        "                name = item.get('name', '')\n",
        "                team = item.get('team', '')\n",
        "                val = item.get('value', '')\n",
        "                if '/' in str(val):\n",
        "                    parts = str(val).split('/')\n",
        "                    match[f\"streak_{team}_{slugify(name)}\"] = int(parts[0])\n",
        "\n",
        "    # H2H (leakage-free)\n",
        "    if include_h2h:\n",
        "        h2h_data = fetch_json(f\"/event/{match_id}/h2h/events\")\n",
        "        if h2h_data and 'events' in h2h_data:\n",
        "            h2h_home = h2h_away = h2h_draws = 0\n",
        "            for h2h in h2h_data.get('events', []):\n",
        "                if h2h.get('startTimestamp', 0) >= match['timestamp']:\n",
        "                    continue\n",
        "                if h2h.get('status', {}).get('type') != 'finished':\n",
        "                    continue\n",
        "                winner = h2h.get('winnerCode')\n",
        "                h2h_home_id = h2h.get('homeTeam', {}).get('id')\n",
        "                if winner == 1:\n",
        "                    if h2h_home_id == match['home_team_id']: h2h_home += 1\n",
        "                    else: h2h_away += 1\n",
        "                elif winner == 2:\n",
        "                    if h2h_home_id == match['home_team_id']: h2h_away += 1\n",
        "                    else: h2h_home += 1\n",
        "                elif winner == 3:\n",
        "                    h2h_draws += 1\n",
        "            match['h2h_home_wins'] = h2h_home\n",
        "            match['h2h_away_wins'] = h2h_away\n",
        "            match['h2h_draws'] = h2h_draws\n",
        "\n",
        "    return match\n",
        "\n",
        "print(\"Data functions ready\")"
      ],
      "metadata": {
        "id": "functions",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87085403-567e-466f-fd3e-564376819eab"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data functions ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_league_parallel(league_name, tournament_id, num_years, max_matches, workers,\n",
        "                           include_odds, include_streaks, include_h2h):\n",
        "    \"\"\"Scrape a single league using parallel workers.\"\"\"\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"SCRAPING: {league_name}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # Get seasons and teams\n",
        "    seasons = get_seasons(tournament_id, num_years)\n",
        "    if not seasons:\n",
        "        print(f\"No seasons found\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    all_teams = {}\n",
        "    for s in seasons[:num_years]:\n",
        "        for t in get_teams(tournament_id, s['id']):\n",
        "            if t['id']: all_teams[t['id']] = t['name']\n",
        "\n",
        "    print(f\"Found {len(seasons)} seasons, {len(all_teams)} teams\")\n",
        "\n",
        "    # Collect all matches (parallel by team)\n",
        "    all_matches = []\n",
        "    team_ids = list(all_teams.keys())\n",
        "\n",
        "    print(f\"Collecting matches from {len(team_ids)} teams...\")\n",
        "    with ThreadPoolExecutor(max_workers=workers) as executor:\n",
        "        futures = {executor.submit(get_team_matches, tid, tournament_id, 3): tid for tid in team_ids}\n",
        "        done = 0\n",
        "        for future in as_completed(futures):\n",
        "            matches = future.result()\n",
        "            all_matches.extend(matches[:max_matches])\n",
        "            done += 1\n",
        "            print(f\"\\r  Teams: {done}/{len(team_ids)} | Matches: {len(all_matches)}\", end='', flush=True)\n",
        "\n",
        "    # Deduplicate\n",
        "    seen = set()\n",
        "    unique_matches = []\n",
        "    for m in all_matches:\n",
        "        if m['match_id'] not in seen:\n",
        "            seen.add(m['match_id'])\n",
        "            m['league'] = league_name\n",
        "            unique_matches.append(m)\n",
        "\n",
        "    print(f\"\\n  Unique matches: {len(unique_matches)}\")\n",
        "\n",
        "    # Enrich matches in parallel\n",
        "    print(f\"Enriching with stats/odds/h2h (parallel)...\")\n",
        "    enriched = []\n",
        "    with ThreadPoolExecutor(max_workers=workers) as executor:\n",
        "        futures = [executor.submit(enrich_match, m, include_odds, include_streaks, include_h2h)\n",
        "                   for m in unique_matches]\n",
        "        done = 0\n",
        "        for future in as_completed(futures):\n",
        "            enriched.append(future.result())\n",
        "            done += 1\n",
        "            if done % 10 == 0:\n",
        "                print(f\"\\r  Enriched: {done}/{len(unique_matches)}\", end='', flush=True)\n",
        "\n",
        "    print(f\"\\n  Completed: {len(enriched)} matches\")\n",
        "    return pd.DataFrame(enriched)\n",
        "\n",
        "print(\"Parallel scraper ready\")"
      ],
      "metadata": {
        "id": "parallel_scraper",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f492492-2106-40f2-f290-e10d667896be"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parallel scraper ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run Scraping { display-mode: \"form\" }\n",
        "\n",
        "all_data = []\n",
        "start_time = time.time()\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"MULTI-LEAGUE PARALLEL SCRAPER\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Leagues: {len(selected)}\")\n",
        "print(f\"Workers: {Parallel_Workers}\")\n",
        "print(f\"Years: {Years_of_Data}\")\n",
        "\n",
        "for name, info in selected.items():\n",
        "    try:\n",
        "        df = scrape_league_parallel(\n",
        "            league_name=name,\n",
        "            tournament_id=info['id'],\n",
        "            num_years=Years_of_Data,\n",
        "            max_matches=Max_Matches_Per_Team,\n",
        "            workers=Parallel_Workers,\n",
        "            include_odds=Include_Odds,\n",
        "            include_streaks=Include_Streaks,\n",
        "            include_h2h=Include_H2H\n",
        "        )\n",
        "        if len(df) > 0:\n",
        "            all_data.append(df)\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError in {name}: {e}\")\n",
        "\n",
        "# Combine\n",
        "if all_data:\n",
        "    df_final = pd.concat(all_data, ignore_index=True)\n",
        "    df_final = df_final.drop_duplicates(subset=['match_id'], keep='first')\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"COMPLETE!\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Total matches: {len(df_final)}\")\n",
        "    print(f\"Columns: {len(df_final.columns)}\")\n",
        "    print(f\"Time: {elapsed/60:.1f} minutes\")\n",
        "    print(f\"API requests: {request_count}\")\n",
        "    print(f\"Leagues: {df_final['league'].nunique()}\")\n",
        "\n",
        "    if 'odds_1x2_home' in df_final.columns:\n",
        "        print(f\"Odds coverage: {df_final['odds_1x2_home'].notna().mean()*100:.1f}%\")\n",
        "else:\n",
        "    df_final = pd.DataFrame()\n",
        "    print(\"No data collected\")"
      ],
      "metadata": {
        "id": "run",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82ce446f-e3ac-4c04-f624-9c8114d88ebb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "MULTI-LEAGUE PARALLEL SCRAPER\n",
            "============================================================\n",
            "Leagues: 6\n",
            "Workers: 10\n",
            "Years: 3\n",
            "\n",
            "==================================================\n",
            "SCRAPING: Premier League\n",
            "==================================================\n",
            "Found 6 seasons, 25 teams\n",
            "Collecting matches from 25 teams...\n",
            "  Teams: 25/25 | Matches: 678\n",
            "  Unique matches: 419\n",
            "Enriching with stats/odds/h2h (parallel)...\n",
            "  Enriched: 410/419\n",
            "  Completed: 419 matches\n",
            "\n",
            "==================================================\n",
            "SCRAPING: La Liga\n",
            "==================================================\n",
            "Found 6 seasons, 26 teams\n",
            "Collecting matches from 26 teams...\n",
            "  Teams: 26/26 | Matches: 697\n",
            "  Unique matches: 440\n",
            "Enriching with stats/odds/h2h (parallel)...\n",
            "  Enriched: 440/440\n",
            "  Completed: 440 matches\n",
            "\n",
            "==================================================\n",
            "SCRAPING: Bundesliga\n",
            "==================================================\n",
            "Found 6 seasons, 21 teams\n",
            "Collecting matches from 21 teams...\n",
            "  Teams: 21/21 | Matches: 611\n",
            "  Unique matches: 374\n",
            "Enriching with stats/odds/h2h (parallel)...\n",
            "  Enriched: 50/374"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview\n",
        "if len(df_final) > 0:\n",
        "    print(\"Sample:\")\n",
        "    display(df_final[['date', 'league', 'home_team', 'away_team', 'home_score', 'away_score']].head(10))\n",
        "\n",
        "    print(\"\\nBy League:\")\n",
        "    print(df_final['league'].value_counts())"
      ],
      "metadata": {
        "id": "preview"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "if len(df_final) > 0:\n",
        "    filename = f\"sofascore_parallel_{len(selected)}lg_{Years_of_Data}yr_{len(df_final)}matches.csv\"\n",
        "    df_final.to_csv(filename, index=False)\n",
        "    print(f\"Saved: {filename}\")\n",
        "    files.download(filename)"
      ],
      "metadata": {
        "id": "download"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Speed Comparison\n",
        "\n",
        "| Workers | Estimated Speed |\n",
        "|---------|----------------|\n",
        "| 1 (serial) | 1x baseline |\n",
        "| 3 workers | ~2.5x faster |\n",
        "| 5 workers | ~4x faster |\n",
        "| 10 workers | ~6x faster (may hit rate limits) |\n",
        "\n",
        "### Tips:\n",
        "- Start with 5 workers\n",
        "- If you get 403 errors, reduce workers\n",
        "- Each league takes ~2-5 minutes with 5 workers"
      ],
      "metadata": {
        "id": "notes"
      }
    }
  ]
}